\section{Peržiūros roboto architektūra}

Šiame skyriuje analizuojamos mokslinėje literatūroje aprašytos saityno peržiūros robotų realizacijos -- pagrindiniai komponentai, jų funkcinės atsakomybės, charakteristikos, atliekama palyginamoji analizė tarp nagrinėjamų peržiūros robotų.

\subsection{Komponentai}

M.Najorc ir C. Olston mokslinė saityno peržiūros sistemų robotų apžvalga (\cite{StanfWebCrawl}) formalizuoja anksčiau literatūroje aprašytas tokių sistemų dizaino specifikas. Joje nusakoma išskirstyto peržiūros roboto architektūra -- skirtingose mašinose egzistuojantys žvalgymo procesai, kiekvienas jų turintis keletą lygiagrečiai veikiančių agentų gijų, kurios atlieka kartotinius žvalgymo ciklo žingsius, kuriuose dalyvauja išskiriami pagrindiniai 8 struktūriniai sistemos komponentai.

\subsubsection{„Pasienis“}

„Pasienio“ duomenų struktūra (angl. -- \textit{URL Frontier}) saugo URL\footnote{URL - Uniform Resource Locator} adresų sąrašą, kurie bus aplankyti, iš šio sąrašo paduodamas adresas žvalgymo agento gijai pagal atitinkamas žvalgymo mandagumo (angl. -- \textit{Politeness}) ir prioritizavimo (angl. -- \textit{Priority}) politikas (\cite{StanfWebCrawl}. Tai viena iš pagrindinių žvalgymo roboto būsenos duomenų struktūrų. Jai keliami šie pagrindiniai funkcionalumo reikalavimai:
\begin{itemize}
    \item Pridėti URL adresą į sąrašą
    \item Nuskaityti URL adresą iš sąrašo
\end{itemize}

\subsubsection{HTTP parsiuntimo modulis}

Žvalgymo agentui gavus URL adresą iškviečiamas HTTP modulis, kuris pirmiausia kreipiasi į \textit{DNS adreso išaiškinimo} komponentą tam, jog būtų nustatytas URL resurso serverio vardo IP protokolo adresas \cite{StanfWebCrawl}. Šis veiksmas reikalingas tam, kad būtų minimizuotas HTTP užklausos atsakymo laikas (išvengiama DNS išaiškinimo užklausų į išorinius serverius).

\subsubsection{Saityno nuorodų ištraukiklis}

Šis komponentas (angl. -- \textit{Link Extractor}) nuskaito parsiųsto HTML dokumento turinį ir išgauna visas HTML nuorodas tiek į išorinius (angl. -- \textit{Offsite Links}), tiek į vidinius (\textit{In-site Links}) žiniatinklio serverio puslapius \cite{StanfWebCrawl}.

\subsubsection{Adresų skirstiklis}

Šis modulis (angl. -- \textit{URL Distributor}) atsakingas už išgautų nuorodų priskyrimą atitinkamiems žvalgymo procesams \cite{StanfWebCrawl}.

\subsubsection{Adresų filtras}

Komponentas, kuris filtruoja priskirtus URL adresus ir gali išmesti taisyklių neatitinkančias nuorodas (pvz.: puslapiai, įtraukti į juodąjį sąrašą) \cite{StanfWebCrawl}. Taisyklės gali būti specializuotos kiekvienam žvalgymui atskirai.

\subsubsection{Dublikatų šalintojas}

Roboto dalis, kuri atlieką testą, ar URL nuoroda dar nebuvo aplankyta peržiūros metu, pagrindiniai keliami funkcionamulo reikalavimai \cite{StanfWebCrawl}:
\begin{itemize}
    \item Pridėti URL adreso aplankymo indikatorių į sąrašą
    \item Atlikti URL priklausymo sąrašui testą
\end{itemize}

\subsubsection{Adresų prioritizuotojas}

Komponentas (angl. -- \textit{URL Prioritizer}), kuris kiekvienam URL adresui priskiria tam tikrą prioritetą pagal specializuotus saityno peržiūros roboto sistemos pasirinkimo politikos faktorius, tokius kaip nustatomas puslapio svarbos laipsnis ar puslapio keitimosi greičio faktorius \cite{StanfWebCrawl}.


\subsection{Peržiūros vykdymo ciklas}

Atsivelgus į 3.1 poskyrio struktūrinius komponentus pagal \cite{StanfWebCrawl} pasiūlytą schematinį dizainą, galima sudaryti veiklos diagramą, parodančią sistemos ciklinį funkcionavimą.

\input{chapters/figures/web_crawler_activity_diagram}

\subsection{Literatūroje aprašytų robotų palyginamoji analizė}

Šiame skyriuje lyginama keletas žinomiausių akademiniuose šaltiniuose aprašytų saityno peržiūros robotų architektūrų siekiant suvokti, kaip kiekviena jų koordinuoja peržiūros proceso agentų darbą.

\subsubsection{Peržiūros robotų išplečiamumo iššūkis}

Nors koncepcinis peržiūros sistemų algoritmas, aprašytas 1 skyriuje, yra labai paprastas, tokių sistemų problemos kyla sprendžiant išplėtimo iššūkius -- siekiant peržiūrėti milijardus svetainių per pagrįstai trumpą laiką ir išlaikyti peržiūrėtų svetainių naujausią galimą kopiją \cite{WCArchitectureMicrosoft}. Pagal \cite{WCArchitectureMicrosoft} apžvalgą, galima būtų iškelti šį pagrindinį peržiūros robotų architektūrų iššūkį:

\begin{itemize}
  \item Sugebėti vykdyti peržiūros procesą išskirstytai ir lygiagrečiai, tačiau vykdyti tai etiškai neapkraunant žiniatinklio serverio (etiško žvalgymo politika)
\end{itemize}

\input{chapters/architectures/polybot}
\input{chapters/architectures/mercator}
\input{chapters/architectures/heritrix}

\pagebreak

Iš \ref{tab:architectures_comparisonl} lentelės apžvelgtų literatūroje pateikiamų klasikinių peržiūros robotų palyginimo matoma, jog aprašyti sprendimai plačiai taiko centralizuotą peržiūros modelį (koordinatorius), kuris didinant lygiagrečiai veikiančių agentų skaičių lemia ribotą greitaveiką, nes yra bendrinis visiems agentams. Taip pat pastebima, jog didžioji dauguma sprendimų nepalaiko išskirstytos tinkle peržiūros koordinacijos (nebent atskiri peržiūros procesai), išskyrus „PolyBot“, kuris teoriškai tą palaiko, tačiau pagal \cite{PolyBotArchitecture} pateikiamą literatūrinę medžiagą, tokia konfigūracija niekada nebuvo išbandyta praktiškai.
\subsubsection{Palyginimo lentelė}

\input{chapters/tables/architectures_comparison}